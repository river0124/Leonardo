CACHE_DIR = "/Users/hyungseoklee/Documents/Leonardo/backend/cache"

from loguru import logger
import json
import FinanceDataReader as fdr # ì´ë†ˆì€ ì“°ë©´ ì•ˆë˜ê² ë‹¤ ë°ì´í„°ê°€ ì •í™•í•˜ì§€ ì•Šì•„.
import pandas as pd
from tqdm import tqdm
import numpy as np
import sys, os
from datetime import datetime, timedelta
import time
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from get_total_data_for_candidates import get_foreign_institution_trend,get_foreign_net_trend
from utils import KoreaInvestAPI, KoreaInvestEnv
from settings import cfg
from slack_notifier import post_to_slack


# ğŸ“Œ ê°•ì„¸ ì„¹í„° íŒë‹¨ ê¸°ì¤€ ì„¤ì •
MIN_PERIOD_DAYS = 20  # ìƒìŠ¹ ê¸°ê°„ ê¸°ì¤€ (ìµœê·¼ N ê±°ë˜ì¼)
STRONG_WINNER_THRESHOLD = 0.07  # ëŒ€ì¥ì£¼ì˜ ìˆ˜ìµë¥ ì´ ì´ ìˆ˜ì¹˜ ì´ìƒì¸ ê²ƒì´ ìˆì–´ì•¼ í•¨
MIN_RATIO_TO_HIGH52 = 0.90  # ì‹ ê³ ê°€ ëŒ€ë¹„ ìµœì†Œ ê·¼ì ‘ ë¹„ìœ¨ (ì˜ˆ: 0.98 = 98%)
MAX_RATIO_DIFF_FROM_HIGH52 = 0.1  # 52ì£¼ ì‹ ê³ ê°€ì—ì„œ ì´ ë¹„ìœ¨ ì´ìƒ ë²—ì–´ë‚œ ì¢…ëª©ì€ ì œì™¸ (ì˜ˆ: 0.10 â†’ 10%)

# Load DEBUG setting
with open(f"{CACHE_DIR}/settings.json") as f:
    settings = json.load(f)
DEBUG = settings.get("DEBUG", "False") == "True"


holidays_path = f"{CACHE_DIR}/holidays.csv"
holidays = pd.read_csv(holidays_path)
holiday_dates = set(pd.to_datetime(holidays['ë‚ ì§œ'], format="%Y%m%d").dt.date)

def remove_holidays(df):
    return df.loc[~df.index.to_series().dt.date.isin(holiday_dates)]

def find_52week_high_candidates():
    # KRX ì¢…ëª© ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    stock_list = fdr.StockListing('KRX')
    stock_list = stock_list[~stock_list['Name'].str.contains('ê´€ë¦¬|ì •ì§€|ìŠ¤íŒ©|ì „í™˜|ìš°$|ìš°[A-Z]?$|ìš°ì„ ', regex=True, na=False)]
    stock_list['Close'] = pd.to_numeric(stock_list['Close'], errors='coerce')
    stock_list = stock_list[stock_list['Close'] > 1000]

    # 52ì£¼ ê¸°ê°„ ì„¤ì •
    end = datetime.today()
    start = end - timedelta(days=365)

    results = []

    for _, row in tqdm(stock_list.iterrows(), total=stock_list.shape[0], desc="ğŸ” 52ì£¼ ì‹ ê³ ê°€ ê²€ìƒ‰ ì¤‘"):
        code = row['Code']
        name = row['Name']

        try:
            df = remove_holidays(fdr.DataReader(code, start, end))
            if df.empty or len(df) < 10:
                continue

            current_close = df['Close'].iloc[-1]
            high_52week = df['High'].max()

            # ğŸ” í˜„ì¬ê°€ê°€ 52ì£¼ ì‹ ê³ ê°€ì˜ MIN_RATIO_TO_HIGH52 ì´ìƒì´ë©´ì„œ MAX_RATIO_DIFF_FROM_HIGH52 ì´ë‚´ì¸ ê²½ìš°ë§Œ í¬í•¨
            ratio = current_close / high_52week
            if MIN_RATIO_TO_HIGH52 <= ratio and ratio >= (1 - MAX_RATIO_DIFF_FROM_HIGH52):
                results.append({
                    'Code': code,
                    'Name': name,
                    'CurrentPrice': current_close,
                    'High52Week': high_52week,
                    'Ratio': round(ratio * 100, 2)
                })

        except Exception as e:
            if DEBUG: logger.error(f"[{code}] {name}: Error -> {e}")

    df_result = pd.DataFrame(results)
    if not df_result.empty:
        df_result = df_result.sort_values(by='Ratio', ascending=False)
    return df_result


# âœ… ì„¹í„°ë³„ ëŒ€ì¥ì£¼ 5ê°œ ì¤‘ ìµœê·¼ 5ì¼ ê°„ ìƒìŠ¹ ì¢…ëª©ì´ 3ê°œ ì´ìƒì¸ ì„¹í„°ë§Œ í•„í„°ë§

import os

stock_list_path = f"{CACHE_DIR}/stock_list_with_sectors.csv"

# ìŠ¤í†¡ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
df_stock_info = pd.read_csv(stock_list_path)
# sector2 â†’ Sector2, sector1 â†’ Sector1 (if present). Ensure both columns exist.
if 'sector1' in df_stock_info.columns:
    df_stock_info = df_stock_info.rename(columns={"sector1": "Sector1"})
if 'sector2' in df_stock_info.columns:
    df_stock_info = df_stock_info.rename(columns={"sector2": "Sector2"})
# Drop rows missing MarketCap or either Sector1 or Sector2
df_stock_info = df_stock_info.dropna(subset=['MarketCap', 'Sector1', 'Sector2'])

# ì„¹í„° í¬ê¸° ë³´ì •: ì¢…ëª© ìˆ˜ 10ê°œ ë¯¸ë§Œ ì„¹í„° ì œì™¸
df_stock_info = df_stock_info.groupby('Sector1').filter(lambda x: len(x) >= 10)
df_stock_info = df_stock_info.groupby('Sector2').filter(lambda x: len(x) >= 10)

# ë™ì  ëŒ€ì¥ì£¼ ê°œìˆ˜ ê²°ì • í•¨ìˆ˜
def dynamic_top_n(sector_size):
    if sector_size <= 5:
        return sector_size
    elif sector_size <= 15:
        return 3
    elif sector_size <= 30:
        return 5
    else:
        return 7

# ì„¹í„°1 ê¸°ë°˜ ëŒ€ì¥ì£¼ (ì‹œê°€ì´ì•¡ ê¸°ì¤€ ìƒìœ„, ë™ì  ê°œìˆ˜)
top_leaders_by_sector1 = (
    df_stock_info
    .sort_values(by="MarketCap", ascending=False)
    .groupby("Sector1")
    .apply(lambda g: g.head(dynamic_top_n(len(g))))
    .reset_index(drop=True)
)

# ì„¹í„°2 ê¸°ë°˜ ëŒ€ì¥ì£¼ (ì‹œê°€ì´ì•¡ ê¸°ì¤€ ìƒìœ„, ë™ì  ê°œìˆ˜)
top_leaders_by_sector2 = (
    df_stock_info
    .sort_values(by="MarketCap", ascending=False)
    .groupby("Sector2")
    .apply(lambda g: g.head(dynamic_top_n(len(g))))
    .reset_index(drop=True)
)

# ê°•ì„¸ ì„¹í„° íŒë‹¨ì„ ìœ„í•œ ì¤€ë¹„
sector1_returns = {}
sector2_returns = {}

# ìµœê·¼ 6ì¼ì¹˜ ë°ì´í„° (ë‹¹ì¼ í¬í•¨ 5ê±°ë˜ì¼)
# ëŒ€í‘œ ì¢…ëª©ìœ¼ë¡œë¶€í„° ê°€ì¥ ìµœê·¼ ê±°ë˜ì¼ êµ¬í•˜ê¸°
ref_df = fdr.DataReader("005930", "2024-01-01")
ref_df = remove_holidays(ref_df)
end_date = ref_df.index[-1].date()
start_date = end_date - timedelta(days=MIN_PERIOD_DAYS * 2)  # ì£¼ë§ê³¼ ê³µíœ´ì¼ í¬í•¨ ì—¬ìœ  í™•ë³´

# ì„¹í„°1 ëŒ€ì¥ì£¼ ìƒìŠ¹ ì²´í¬
for _, row in tqdm(top_leaders_by_sector1.iterrows(), total=top_leaders_by_sector1.shape[0], desc="ğŸ“Š Sector1 ëŒ€ì¥ì£¼ ìƒìŠ¹ ì²´í¬"):
    code = str(row["Code"]).zfill(6)
    sector = row["Sector1"]
    try:
        df = remove_holidays(fdr.DataReader(code, start_date, end_date.strftime('%Y-%m-%d')))
        df = df.tail(MIN_PERIOD_DAYS + 1)
        if len(df) < MIN_PERIOD_DAYS + 1:
            continue
        ret = (df['Close'].iloc[-1] - df['Close'].iloc[0]) / df['Close'].iloc[0]
        sector1_returns.setdefault(sector, []).append(ret)
    except:
        continue

# ì„¹í„°2 ëŒ€ì¥ì£¼ ìƒìŠ¹ ì²´í¬
for _, row in tqdm(top_leaders_by_sector2.iterrows(), total=top_leaders_by_sector2.shape[0], desc="ğŸ“Š Sector2 ëŒ€ì¥ì£¼ ìƒìŠ¹ ì²´í¬"):
    code = str(row["Code"]).zfill(6)
    sector = row["Sector2"]
    try:
        df = remove_holidays(fdr.DataReader(code, start_date, end_date.strftime('%Y-%m-%d')))
        df = df.tail(MIN_PERIOD_DAYS + 1)
        if len(df) < MIN_PERIOD_DAYS + 1:
            continue
        ret = (df['Close'].iloc[-1] - df['Close'].iloc[0]) / df['Close'].iloc[0]
        sector2_returns.setdefault(sector, []).append(ret)
    except:
        continue

# âœ… ê°•ì„¸ ì„¹í„° ì •ì˜ (Sector1, Sector2 ê°ê°, ìƒˆë¡œìš´ ë°©ì‹)

# Stricter filtering for Sector1
def calculate_strong_sector1(sector_returns_dict):
    strong_sectors = []
    for sector, returns in sector_returns_dict.items():
        if len(returns) < 2:
            continue
        # Skip if the maximum return is less than 0.03
        if max(returns) < 0.03:
            continue
        avg_return = sum(returns) / len(returns)
        rising_ratio = sum(r > 0 for r in returns) / len(returns)
        strong_ratio = sum(r >= STRONG_WINNER_THRESHOLD for r in returns) / len(returns)
        score = (avg_return * 100) + (rising_ratio * 20) + (strong_ratio * 30) - np.log(len(returns)) * 2
        pass_count = sum([
            avg_return >= 0.04,
            rising_ratio >= 0.7,
            strong_ratio >= 0.35,
            score >= 3.0
        ])
        if pass_count >= 3:
            strong_sectors.append(sector)
    return strong_sectors

# More relaxed for Sector2
def calculate_strong_sector2(sector_returns_dict):
    strong_sectors = []
    for sector, returns in sector_returns_dict.items():
        if len(returns) < 2:
            continue
        # Skip if the maximum return is less than 0.03
        if max(returns) < 0.03:
            continue
        avg_return = sum(returns) / len(returns)
        rising_ratio = sum(r > 0 for r in returns) / len(returns)
        strong_ratio = sum(r >= STRONG_WINNER_THRESHOLD for r in returns) / len(returns)
        score = (avg_return * 100) + (rising_ratio * 20) + (strong_ratio * 30) - np.log(len(returns)) * 2
        pass_count = sum([
            avg_return >= 0.03,
            rising_ratio >= 0.65,
            strong_ratio >= 0.25,
            score >= 2.5
        ])
        if pass_count >= 2:
            strong_sectors.append(sector)
    return strong_sectors

strong_sector1 = calculate_strong_sector1(sector1_returns)
strong_sector2 = calculate_strong_sector2(sector2_returns)

# ğŸ”¥ ìƒìŠ¹ ì¤‘ì¸ ê°•ì„¸ ì„¹í„° ëª©ë¡ ë¡œê¹… (Sector1, Sector2)
if DEBUG:
    logger.info(f"ğŸ”¥ ìƒìŠ¹ ì¤‘ì¸ ê°•ì„¸ ì„¹í„° ëª©ë¡ - Sector1: {strong_sector1}")
    logger.info(f"ğŸ”¥ ìƒìŠ¹ ì¤‘ì¸ ê°•ì„¸ ì„¹í„° ëª©ë¡ - Sector2: {strong_sector2}")

def merge_sector_info(df_result, df_stock_info):
    df_result['Code'] = df_result['Code'].astype(str).str.zfill(6)
    df_stock_info['Code'] = df_stock_info['Code'].astype(str).str.zfill(6)
    return df_result.merge(df_stock_info[['Code', 'Sector1', 'Sector2']], on='Code', how='left')

def filter_by_strong_sector(df_result, strong_sector1, strong_sector2):
    return df_result[
        df_result['Sector1'].isin(strong_sector1) |
        df_result['Sector2'].isin(strong_sector2)
    ]

def filter_small_caps(df_result, df_stock_info):
    df_result = df_result.merge(df_stock_info[['Code', 'Market', 'MarketCap']], on='Code', how='left')
    df_result = df_result[
        ((df_result['Market'] == 'KOSPI') & (df_result['MarketCap'] >= 500000000000)) |
        ((df_result['Market'] == 'KOSDAQ') & (df_result['MarketCap'] >= 300000000000))
    ]
    return df_result

# Sector1, Sector2 ëª¨ë‘ì— ì†í•œ ì¢…ëª©ì€ ì¶”ê°€ ì ìˆ˜ë¥¼ ë°›ì•„ ìƒìœ„ë¡œ ì •ë ¬ë˜ë„ë¡ ì ìˆ˜í™” í•¨ìˆ˜ (ì ìˆ˜ ë°©ì‹ ë³€ê²½)
def score_strong_sector(df_result, strong_sector1, strong_sector2):
    def calc_score(row):
        s1 = row['Sector1'] in strong_sector1
        s2 = row['Sector2'] in strong_sector2
        return s1 * 2 + s2 * 2 + (s1 and s2) * 1  # ì´ì  ìµœëŒ€ 5ì 

    df_result['SectorScore'] = df_result.apply(calc_score, axis=1)
    df_result = df_result[df_result['SectorScore'] >= 2.5]
    df_result = df_result.sort_values(by='SectorScore', ascending=False)
    return df_result

def get_foreign_institution_trend(stock_code):
    original_mode = settings.get("is_paper_trading", True)

    if original_mode:
        cfg["is_paper_trading"] = False
    else:
        # logger.info("í˜„ì¬ëŠ” ì‹¤ì „íˆ¬ì ìƒíƒœì…ë‹ˆë‹¤.")
        pass

    env = KoreaInvestEnv(cfg)
    api = KoreaInvestAPI(cfg, env.get_base_headers())

    response = api.summarize_foreign_institution_estimates(stock_code)
    response_json = response.json()
    output2 = response_json.get("output2", [])

    if output2:
        # ì‹œê°„ëŒ€ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        latest = max(output2, key=lambda x: int(x["bsop_hour_gb"]))
        frgn = int(latest["frgn_fake_ntby_qty"])
        orgn = int(latest["orgn_fake_ntby_qty"])

        return {"ì™¸êµ­ì¸": frgn, "ê¸°ê´€": orgn}
    else:
        return {"ì™¸êµ­ì¸": 0, "ê¸°ê´€": 0}

def get_foreign_net_trend(stock_code):
    original_mode = settings.get("is_paper_trading", True)

    if original_mode:
        cfg["is_paper_trading"] = False
    else:
        # logger.info("í˜„ì¬ëŠ” ì‹¤ì „íˆ¬ì ìƒíƒœì…ë‹ˆë‹¤.")
        pass

    env = KoreaInvestEnv(cfg)
    api = KoreaInvestAPI(cfg, env.get_base_headers())

    response = api.summarize_foreign_net_estimates(stock_code)
    response_json = response.json()
    output = response_json.get("output", [])

    if output:
        # ì‹œê°„ëŒ€ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        latest = max(output, key=lambda x: int(x["bsop_hour"]))
        glob_ntby_qty = int(latest["glob_ntby_qty"])
        return {"ì™¸êµ­ê³„": glob_ntby_qty}
    else:
        return {"ì™¸êµ­ê³„": 0}

def get_total_trading_data(stock_code):
    env = KoreaInvestEnv(cfg)
    api = KoreaInvestAPI(cfg, env.get_base_headers())

    response = api.get_current_price(stock_code)
    output = response["output"][0] if isinstance(response.get("output"), list) else response

    # ëˆ„ì ê±°ë˜ëŸ‰ ì¶”ì¶œ ë° ë§¤í•‘
    acml_vol = output.get("acml_vol")

    try:
        return {"ëˆ„ì ê±°ë˜ëŸ‰": int(acml_vol)} if acml_vol is not None else {"ëˆ„ì ê±°ë˜ëŸ‰": 0}
    except ValueError:
        return {"ëˆ„ì ê±°ë˜ëŸ‰": 0}

if __name__ == "__main__":
    df_result = find_52week_high_candidates()
    df_result = merge_sector_info(df_result, df_stock_info)

    # ğŸš« ê°•ì„¸ ì„¹í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if not strong_sector1 and not strong_sector2:
        if DEBUG: logger.info("âš ï¸ ê°•ì„¸ ì„¹í„°ê°€ ì—†ì–´ ì¶”ì²œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        post_to_slack("âš ï¸ ê°•ì„¸ ì„¹í„°ê°€ ì—†ì–´ ì¶”ì²œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()

    df_result = filter_by_strong_sector(df_result, strong_sector1, strong_sector2)
    df_result = filter_small_caps(df_result, df_stock_info)
    df_result = score_strong_sector(df_result, strong_sector1, strong_sector2)
    df_result = df_result.sort_values(by='SectorScore', ascending=False)
    added_count = len(df_result)
    if DEBUG:
        logger.info(f"âœ… ê°•ì„¸ ì„¹í„° í•„í„°ë§ í›„ ì¶”ê°€ëœ ì¢…ëª© ìˆ˜: {added_count}")

    # âœ… ì™¸êµ­ì¸, ê¸°ê´€, ì™¸êµ­ê³„ ë§¤ìˆ˜ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ë° ì¶”ê°€
    foreign_orgn_list = []
    foreign_net_list = []
    volume_list = []

    for _, row in tqdm(df_result.iterrows(), total=len(df_result), desc="ğŸŒ ì™¸êµ­ì¸/ê¸°ê´€/ì™¸êµ­ê³„ ë§¤ìˆ˜ëŸ‰ ì¡°íšŒ ì¤‘"):
        code = row["Code"]
        try:
            trend = get_foreign_institution_trend(code)
            # í•„í„°: ì™¸êµ­ì¸ ë˜ëŠ” ê¸°ê´€ ìˆœë§¤ìˆ˜ ìŒìˆ˜ë©´ ì œì™¸
            if trend["ì™¸êµ­ì¸"] < 0 or trend["ê¸°ê´€"] < 0:
                # logger.debug(f"[FILTERED] {code} ì œì™¸ë¨ - ì™¸êµ­ì¸ ë˜ëŠ” ê¸°ê´€ ìˆœë§¤ìˆ˜ ìŒìˆ˜")
                continue
            # í•„í„°: ì£¼ì„ìœ¼ë¡œ ì œê±° ê°€ëŠ¥
            time.sleep(0.4)
            net = get_foreign_net_trend(code)
            time.sleep(0.4)
            volume = get_total_trading_data(code)
            time.sleep(0.4)
        except Exception as e:
            logger.warning(f"[{code}] ì™¸êµ­ì¸/ê¸°ê´€/ì™¸êµ­ê³„ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            trend = {"ì™¸êµ­ì¸": 0, "ê¸°ê´€": 0}
            net = {"ì™¸êµ­ê³„": 0}
            volume = {"ëˆ„ì ê±°ë˜ëŸ‰": 0}

        foreign_orgn_list.append(trend)
        foreign_net_list.append(net)
        volume_list.append(volume)

    # ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ê³  df_resultì— ë³‘í•©
    df_foreign_orgn = pd.DataFrame(foreign_orgn_list)
    df_foreign_net = pd.DataFrame(foreign_net_list)
    df_volume = pd.DataFrame(volume_list)
    df_result = pd.concat([df_result.reset_index(drop=True), df_foreign_orgn, df_foreign_net, df_volume], axis=1)

    def refined_score(row):
        total_volume = row["ëˆ„ì ê±°ë˜ëŸ‰"]
        if total_volume <= 0:
            return 0

        score = 0
        weights = {}

        # âœ… 1. ê°œë³„ ì£¼ì²´ ìŠ¤ì½”ì–´ë§
        for key in ["ê¸°ê´€", "ì™¸êµ­ì¸", "ì™¸êµ­ê³„"]:
            buy = row.get(key, 0)
            ratio = max(0, buy / total_volume)
            multiplier = 1.0
            if key == "ì™¸êµ­ì¸" and row.get("Market") == "KOSPI":
                multiplier = 1.2
            weights[key] = min(3, round(np.log1p(ratio) * 5 * multiplier, 2)) if buy > 0 else 0
            score += weights[key]

        # âœ… 2. ì–‘ë§¤ìˆ˜ ì¡°ê±´
        orgn_buy = row.get("ê¸°ê´€", 0)
        frgn_buy = row.get("ì™¸êµ­ì¸", 0)
        if orgn_buy > 10000000 and frgn_buy > 10000000:
            score += 2
        elif orgn_buy > 0 and frgn_buy > 0:
            score += 1
        elif orgn_buy > 0 or frgn_buy > 0:
            score += 0.5

        # âœ… 3. ì´ë§¤ìˆ˜ ê°•ë„
        total_buy = max(0, orgn_buy + frgn_buy + row.get("ì™¸êµ­ê³„", 0))
        ratio = total_buy / total_volume
        score += min(3.0, round(np.log1p(ratio * 100), 2))

        return round(score, 3)

    df_result["BuyStrengthScore"] = df_result.apply(refined_score, axis=1)
    df_result = df_result.sort_values(by="BuyStrengthScore", ascending=False)
    df_result_top10 = df_result.head(10)

    # âœ… ìµœì¢… ê²°ê³¼ ì €ì¥
    output_path = f"{CACHE_DIR}/high52.json"
    df_result = df_result.drop(columns=['MarketCap'], errors='ignore')
    logger.info(f"ğŸ“¦ ìµœì¢… ì €ì¥í•  ì¢…ëª© ìˆ˜: {len(df_result)}ê°œ")
    df_result.to_json(output_path, orient='records', force_ascii=False, indent=2)
    post_to_slack(f"âœ… ê°•ì„¸ ì„¹í„° ë‚´ ì¶”ì²œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                  f"Sector1: {len(strong_sector1)}ê°œ, Sector2: {len(strong_sector2)}ê°œ\n"
                  f"ì´ ì¢…ëª© ìˆ˜: {len(df_result)}ê°œ")

    top10_path = f"{CACHE_DIR}/high52_top10.json"
    df_result_top10.to_json(top10_path, orient='records', force_ascii=False, indent=2)
    # ğŸ… ìƒìœ„ ì¶”ì²œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ (Top 10)
    message = ["ğŸ… ìƒìœ„ ì¶”ì²œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ (Top 10):"]

    for idx, row in df_result_top10.iterrows():
        total_volume = row.get('ëˆ„ì ê±°ë˜ëŸ‰', 0)
        frgn = row.get('ì™¸êµ­ì¸', 0)
        orgn = row.get('ê¸°ê´€', 0)
        glob = row.get('ì™¸êµ­ê³„', 0)

        def pct(value):
            return f"{value:,} ({(value / total_volume * 100):.1f}%)" if total_volume > 0 else f"{value:,} (0%)"

        msg = (
            f"{row['Name']} ({row['Code']}): "
            f"Score = {row['BuyStrengthScore']} | "
            f"ì™¸êµ­ì¸ = {pct(frgn)}, ê¸°ê´€ = {pct(orgn)}, ì™¸êµ­ê³„ = {pct(glob)}"
        )
        logger.info(msg)
        message.append(msg)

    post_to_slack("\n".join(message))